{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12287144,"sourceType":"datasetVersion","datasetId":7743669},{"sourceId":12315409,"sourceType":"datasetVersion","datasetId":7762636}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT-2 + Morph + Phoneme + SAT","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nfrom typing import List\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:11.108551Z","iopub.execute_input":"2026-01-01T17:06:11.109186Z","iopub.status.idle":"2026-01-01T17:06:22.199196Z","shell.execute_reply.started":"2026-01-01T17:06:11.109158Z","shell.execute_reply":"2026-01-01T17:06:22.198209Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\n\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\nprint(\"Samples:\", len(train_ds), len(eval_ds))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:22.201044Z","iopub.execute_input":"2026-01-01T17:06:22.201306Z","iopub.status.idle":"2026-01-01T17:06:22.251041Z","shell.execute_reply.started":"2026-01-01T17:06:22.201279Z","shell.execute_reply":"2026-01-01T17:06:22.250341Z"}},"outputs":[{"name":"stdout","text":"Samples: 1603 283\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# TOKENIZATION","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nSPECIAL_TOKENS = {\n    \"additional_special_tokens\": [\n        \"<|dialect|>\",\n        \"<|source|>\",\n        \"<|target|>\"\n    ]\n}\n\ntokenizer.add_special_tokens(SPECIAL_TOKENS)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nMAX_LEN = 256\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:22.251980Z","iopub.execute_input":"2026-01-01T17:06:22.252395Z","iopub.status.idle":"2026-01-01T17:06:22.540599Z","shell.execute_reply.started":"2026-01-01T17:06:22.252361Z","shell.execute_reply":"2026-01-01T17:06:22.539927Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# TRAINING (FAIR COMPARISON)","metadata":{}},{"cell_type":"code","source":"class MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n        self.model = BertModel.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        chars = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            chars.append(\n                self.char_emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n\n        return torch.cat([bert, torch.stack(chars)], dim=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:22.542237Z","iopub.execute_input":"2026-01-01T17:06:22.542468Z","iopub.status.idle":"2026-01-01T17:06:22.549489Z","shell.execute_reply.started":"2026-01-01T17:06:22.542445Z","shell.execute_reply":"2026-01-01T17:06:22.548659Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# BLEU EVALUATION (CORRECT)","metadata":{}},{"cell_type":"code","source":"class PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n\n        phones = \"pbtdkgmnshlrwjiɛaəɔou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(\n                self.emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n        return torch.stack(outs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:22.550385Z","iopub.execute_input":"2026-01-01T17:06:22.550620Z","iopub.status.idle":"2026-01-01T17:06:22.563962Z","shell.execute_reply.started":"2026-01-01T17:06:22.550586Z","shell.execute_reply":"2026-01-01T17:06:22.563325Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"class SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, mask):\n        B, L, H = h.size()\n\n        pos = torch.arange(L, device=h.device)\n        rel = pos[None, :] - pos[:, None] + 64\n        rel = rel.clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1, 2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(mask[:, None, :] == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        return torch.bmm(attn, v)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:22.564904Z","iopub.execute_input":"2026-01-01T17:06:22.565142Z","iopub.status.idle":"2026-01-01T17:06:22.577432Z","shell.execute_reply.started":"2026-01-01T17:06:22.565121Z","shell.execute_reply":"2026-01-01T17:06:22.576836Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat  = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n\n        texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n\n        m = self.m_proj(self.morph(texts)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(texts)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n\nmodel = GPT2FusionSAT().to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:22.578415Z","iopub.execute_input":"2026-01-01T17:06:22.578719Z","iopub.status.idle":"2026-01-01T17:06:27.098814Z","shell.execute_reply.started":"2026-01-01T17:06:22.578687Z","shell.execute_reply":"2026-01-01T17:06:27.098199Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def preprocess(batch):\n    ids, masks, labels = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN\n        )\n\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\n        \"input_ids\": ids,\n        \"attention_mask\": masks,\n        \"labels\": labels\n    }\n\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:27.099745Z","iopub.execute_input":"2026-01-01T17:06:27.099983Z","iopub.status.idle":"2026-01-01T17:06:27.770314Z","shell.execute_reply.started":"2026-01-01T17:06:27.099961Z","shell.execute_reply":"2026-01-01T17:06:27.769544Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdcdf13f8a88402ebc0a45e39a7b0e91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/283 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9814e102be4647bc877f47a3e5b01ca8"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"./fusion-sat\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=3e-5,\n    max_steps=800,\n    logging_steps=50,\n    save_strategy=\"no\",\n    evaluation_strategy=\"no\",\n    report_to=\"none\",\n    fp16=torch.cuda.is_available()\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:06:27.771278Z","iopub.execute_input":"2026-01-01T17:06:27.771474Z","iopub.status.idle":"2026-01-01T17:28:44.638825Z","shell.execute_reply.started":"2026-01-01T17:06:27.771455Z","shell.execute_reply":"2026-01-01T17:28:44.638171Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [800/800 22:14, Epoch 7/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>3.829000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.625000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.537400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.456600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.397900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.341300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.336500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.315500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.304900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.305800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.287600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.290600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.291300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.279000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.274800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.280200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=800, training_loss=0.5720749282836914, metrics={'train_runtime': 1336.1902, 'train_samples_per_second': 9.579, 'train_steps_per_second': 0.599, 'total_flos': 0.0, 'train_loss': 0.5720749282836914, 'epoch': 7.98})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw_data, n=100):\n    model.eval()\n    preds, refs = [], []\n\n    for ex in raw_data[:n]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.model.generate(\n                **enc,\n                max_new_tokens=50,\n                num_beams=4,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n\n    return bleu.corpus_score(preds, [refs]).score\n\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:28:44.640791Z","iopub.execute_input":"2026-01-01T17:28:44.641043Z","iopub.status.idle":"2026-01-01T17:29:48.357818Z","shell.execute_reply.started":"2026-01-01T17:28:44.640996Z","shell.execute_reply":"2026-01-01T17:29:48.357073Z"}},"outputs":[{"name":"stdout","text":"BLEU: 9.316781995091635\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def translate_input(model, dialect, source):\n    prompt = f\"<|dialect|> {dialect}\\n<|source|> {source}\\n<|target|> \"\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        out = model.model.generate(\n            **enc,\n            max_new_tokens=40,\n            num_beams=4,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text.split(\"<|target|>\")[-1].strip()\n\n\nprint(translate_input(model, \"kelantanese\", \"kuat\"))\nprint(translate_input(model, \"manglish\", \"I tak tau lah\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T17:29:48.358658Z","iopub.execute_input":"2026-01-01T17:29:48.358944Z","iopub.status.idle":"2026-01-01T17:29:49.389929Z","shell.execute_reply.started":"2026-01-01T17:29:48.358914Z","shell.execute_reply":"2026-01-01T17:29:49.389198Z"}},"outputs":[{"name":"stdout","text":"kelantanese\n kuat\n ampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampampamp\nmanglish\n I tak tau lah\n ikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikikik\n","output_type":"stream"}],"execution_count":19}]}