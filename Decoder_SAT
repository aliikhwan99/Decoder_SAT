{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12287144,"sourceType":"datasetVersion","datasetId":7743669},{"sourceId":12315409,"sourceType":"datasetVersion","datasetId":7762636}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================\n# 0Ô∏è‚É£ ENVIRONMENT (MATCHED VERSIONS ‚Äì IMPORTANT)\n# =========================================================\n!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate==0.26.1 datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nfrom typing import List\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1Ô∏è‚É£ LOAD DATA\n# =========================================================\ndef load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\n# =========================================================\n# 2Ô∏è‚É£ TOKENIZER (LEFT PAD ‚Äì REQUIRED FOR GPT)\n# =========================================================\nMODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntokenizer.add_special_tokens({\n    \"additional_special_tokens\": [\n        \"<|dialect|>\",\n        \"<|source|>\",\n        \"<|target|>\"\n    ]\n})\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nMAX_LEN = 256\n\n# =========================================================\n# 3Ô∏è‚É£ MORPHOLOGY EMBEDDER (FROZEN)\n# =========================================================\nclass MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n        self.model = BertModel.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        char_feats = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            char_feats.append(\n                self.char_emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n\n        return torch.cat([bert, torch.stack(char_feats)], dim=-1)\n\n# =========================================================\n# 4Ô∏è‚É£ PHONEME EMBEDDER\n# =========================================================\nclass PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n\n        phones = \"pbtdkgmnshlrwji…õa…ô…îou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(\n                self.emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n        return torch.stack(outs)\n\n# =========================================================\n# 5Ô∏è‚É£ UNSUPERVISED SYNTAX-AWARE ATTENTION\n# =========================================================\nclass SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, mask):\n        B, L, H = h.size()\n\n        pos = torch.arange(L, device=h.device)\n        rel = pos[None, :] - pos[:, None] + 64\n        rel = rel.clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1, 2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(mask[:, None, :] == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        return torch.bmm(attn, v)\n\n# =========================================================\n# 6Ô∏è‚É£ GPT-2 FUSION + SAT MODEL (FIXED)\n# =========================================================\nclass GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat  = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n\n        # üî• CRITICAL FIX: SOURCE-ONLY TEXT\n        decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n        texts = []\n        for t in decoded:\n            if \"<|source|>\" in t:\n                t = t.split(\"<|source|>\")[1].split(\"<|target|>\")[0]\n            texts.append(t.strip())\n\n        m = self.m_proj(self.morph(texts)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(texts)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\nmodel = GPT2FusionSAT().to(device)\n\n# =========================================================\n# 7Ô∏è‚É£ DATA PREPROCESSING\n# =========================================================\ndef preprocess(batch):\n    ids, masks, labels = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN\n        )\n\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\n        \"input_ids\": ids,\n        \"attention_mask\": masks,\n        \"labels\": labels\n    }\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n\n# =========================================================\n# 8Ô∏è‚É£ TRAINING\n# =========================================================\nargs = TrainingArguments(\n    output_dir=\"./fusion-sat\",\n\n    # ====== BATCHING ======\n    per_device_train_batch_size=2,     # keep small (SAT is expensive)\n    gradient_accumulation_steps=8,      # effective batch = 16\n\n    # ====== OPTIMIZATION ======\n    learning_rate=2e-5,                 # lower LR for fusion + SAT\n    warmup_steps=800,                   # VERY important for SAT\n    max_steps=4000,                     # üöÄ huge training\n\n    # ====== STABILITY ======\n    lr_scheduler_type=\"cosine\",         # smoother than linear\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n\n    # ====== LOGGING ======\n    logging_steps=100,\n\n    # ====== CHECKPOINTING ======\n    save_strategy=\"no\",                 # avoid safetensors issues\n    evaluation_strategy=\"no\",\n\n    # ====== SYSTEM ======\n    report_to=\"none\",\n    fp16=torch.cuda.is_available(),\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\n# =========================================================\n# 9Ô∏è‚É£ BLEU EVALUATION\n# =========================================================\nfrom sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw_data, n=100):\n    model.eval()\n    preds, refs = [], []\n\n    for ex in raw_data[:n]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.model.generate(\n                **enc,\n                max_new_tokens=50,\n                num_beams=4,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n\n    return bleu.corpus_score(preds, [refs]).score\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw))\n\n# =========================================================\n# üîü INFERENCE (NO REPETITION)\n# =========================================================\ndef translate_input(model, dialect, source):\n    prompt = f\"<|dialect|> {dialect}\\n<|source|> {source}\\n<|target|> \"\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        out = model.model.generate(\n            **enc,\n            max_new_tokens=40,\n            do_sample=True,\n            temperature=0.8,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text.split(\"<|target|>\")[-1].strip()\n\nprint(translate_input(model, \"kelantanese\", \"kuat\"))\nprint(translate_input(model, \"manglish\", \"I tak tau lah\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FULL CODE V2","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# 0Ô∏è‚É£ ENVIRONMENT (MATCHED & STABLE)\n# =========================================================\n!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate==0.26.1 datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1Ô∏è‚É£ LOAD DATA (YOUR DIRECTORIES)\n# =========================================================\ndef load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\nprint(\"Train / Eval:\", len(train_ds), len(eval_ds))\n\n# =========================================================\n# 2Ô∏è‚É£ TOKENIZER (LEFT PAD ‚Äì REQUIRED)\n# =========================================================\nMODEL_NAME = \"gpt2\"\nMAX_LEN = 256\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.add_special_tokens({\n    \"additional_special_tokens\": [\"<|dialect|>\", \"<|source|>\", \"<|target|>\"]\n})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\n# =========================================================\n# 3Ô∏è‚É£ MORPHOLOGY EMBEDDER (FROZEN)\n# =========================================================\nclass MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        self.model = BertModel.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts, padding=True, truncation=True,\n            max_length=128, return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        chars = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            chars.append(self.char_emb(torch.tensor(ids, device=device)).mean(dim=0))\n\n        return torch.cat([bert, torch.stack(chars)], dim=-1)\n\n# =========================================================\n# 4Ô∏è‚É£ PHONEME EMBEDDER\n# =========================================================\nclass PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n\n        phones = \"pbtdkgmnshlrwji…õa…ô…îou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(self.emb(torch.tensor(ids, device=device)).mean(dim=0))\n        return torch.stack(outs)\n\n# =========================================================\n# 5Ô∏è‚É£ UNSUPERVISED SYNTAX-AWARE ATTENTION\n# =========================================================\nclass SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, mask):\n        B, L, H = h.size()\n        pos = torch.arange(L, device=h.device)\n        rel = (pos[None, :] - pos[:, None] + 64).clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1, 2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(mask[:, None, :] == 0, -1e9)\n\n        return torch.bmm(torch.softmax(scores, dim=-1), v)\n\n# =========================================================\n# 6Ô∏è‚É£ GPT-2 FUSION + SAT MODEL\n# =========================================================\nclass GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat  = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n\n        decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n        sources = []\n        for t in decoded:\n            if \"<|source|>\" in t:\n                t = t.split(\"<|source|>\")[1].split(\"<|target|>\")[0]\n            sources.append(t.strip())\n\n        m = self.m_proj(self.morph(sources)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(sources)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\nmodel = GPT2FusionSAT().to(device)\n\n# =========================================================\n# 7Ô∏è‚É£ PREPROCESS + SAFE COLLATE (FIXED)\n# =========================================================\ndef preprocess(batch):\n    ids, masks, labels = [], [], []\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(full, truncation=True, max_length=MAX_LEN)\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\"input_ids\": ids, \"attention_mask\": masks, \"labels\": labels}\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ndef collate_fn(batch):\n    def pad(seq, val): return seq + [val] * (MAX_LEN - len(seq))\n\n    return {\n        \"input_ids\": torch.tensor([pad(b[\"input_ids\"], tokenizer.pad_token_id) for b in batch]),\n        \"attention_mask\": torch.tensor([pad(b[\"attention_mask\"], 0) for b in batch]),\n        \"labels\": torch.tensor([pad(b[\"labels\"], -100) for b in batch])\n    }\n\n# =========================================================\n# 8Ô∏è‚É£ TRAINING\n# =========================================================\nargs = TrainingArguments(\n    output_dir=\"./fusion-sat\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    max_steps=4000,\n    logging_steps=100,\n    save_strategy=\"no\",\n    report_to=\"none\",\n    fp16=torch.cuda.is_available()\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    data_collator=collate_fn\n)\n\ntrainer.train()\n\n# =========================================================\n# 9Ô∏è‚É£ BLEU EVALUATION\n# =========================================================\nfrom sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw, n=100):\n    model.eval()\n    preds, refs = [], []\n    for ex in raw[:n]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            out = model.model.generate(**enc, max_new_tokens=40)\n        pred = tokenizer.decode(out[0], skip_special_tokens=True).split(\"<|target|>\")[-1].strip()\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n    return bleu.corpus_score(preds, [refs]).score\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw))\n\n# =========================================================\n# üîü INFERENCE\n# =========================================================\ndef translate(dialect, src):\n    p = f\"<|dialect|> {dialect}\\n<|source|> {src}\\n<|target|> \"\n    enc = tokenizer(p, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        out = model.model.generate(\n            **enc,\n            max_new_tokens=40,\n            do_sample=True,\n            top_p=0.9,\n            temperature=0.8,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=3\n        )\n    return tokenizer.decode(out[0], skip_special_tokens=True).split(\"<|target|>\")[-1].strip()\n\nprint(translate(\"kelantanese\", \"kuat\"))\nprint(translate(\"manglish\", \"I tak tau lah\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# -------------","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# 0Ô∏è‚É£ ENVIRONMENT (MATCHED & STABLE)\n# =========================================================\n!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate==0.26.1 datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1Ô∏è‚É£ LOAD DATA (YOUR DIRECTORIES)\n# =========================================================\ndef load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\nprint(\"Train / Eval:\", len(train_ds), len(eval_ds))\n\n# =========================================================\n# 2Ô∏è‚É£ TOKENIZER (LEFT PAD ‚Äì REQUIRED)\n# =========================================================\nMODEL_NAME = \"gpt2\"\nMAX_LEN = 256\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.add_special_tokens({\n    \"additional_special_tokens\": [\n        \"<|dialect|>\", \"<|source|>\", \"<|target|>\",\n        \"<|task_manglish|>\", \"<|task_kelantanese|>\"\n    ]\n})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\n# =========================================================\n# 3Ô∏è‚É£ MORPHOLOGY EMBEDDER (FROZEN)\n# =========================================================\nclass MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        self.model = BertModel.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts, padding=True, truncation=True,\n            max_length=128, return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        chars = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            chars.append(self.char_emb(torch.tensor(ids, device=device)).mean(dim=0))\n\n        return torch.cat([bert, torch.stack(chars)], dim=-1)\n\n# =========================================================\n# 4Ô∏è‚É£ PHONEME EMBEDDER\n# =========================================================\nclass PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n\n        phones = \"pbtdkgmnshlrwji…õa…ô…îou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(self.emb(torch.tensor(ids, device=device)).mean(dim=0))\n        return torch.stack(outs)\n\n# =========================================================\n# 5Ô∏è‚É£ UNSUPERVISED SYNTAX-AWARE ATTENTION\n# =========================================================\nclass SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, mask):\n        B, L, H = h.size()\n        pos = torch.arange(L, device=h.device)\n        rel = (pos[None, :] - pos[:, None] + 64).clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1, 2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(mask[:, None, :] == 0, -1e9)\n\n        return torch.bmm(torch.softmax(scores, dim=-1), v)\n\n# =========================================================\n# 6Ô∏è‚É£ GPT-2 FUSION + SAT MODEL\n# =========================================================\nclass GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat  = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n\n        decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n        sources = []\n        for t in decoded:\n            if \"<|source|>\" in t:\n                t = t.split(\"<|source|>\")[1].split(\"<|target|>\")[0]\n            sources.append(t.strip())\n\n        m = self.m_proj(self.morph(sources)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(sources)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\nmodel = GPT2FusionSAT().to(device)\n\n# =========================================================\n# 7Ô∏è‚É£ PREPROCESS + SAFE COLLATE (TASK TOKENS)\n# =========================================================\ndef preprocess(batch):\n    ids, masks, labels = [], [], []\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        # ‚ö° Explicit task token\n        task_token = \"<|task_manglish|>\" if d == \"manglish\" else \"<|task_kelantanese|>\"\n\n        prompt = f\"{task_token}\\n<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(full, truncation=True, max_length=MAX_LEN)\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\"input_ids\": ids, \"attention_mask\": masks, \"labels\": labels}\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ndef collate_fn(batch):\n    def pad(seq, val): return seq + [val] * (MAX_LEN - len(seq))\n\n    return {\n        \"input_ids\": torch.tensor([pad(b[\"input_ids\"], tokenizer.pad_token_id) for b in batch]),\n        \"attention_mask\": torch.tensor([pad(b[\"attention_mask\"], 0) for b in batch]),\n        \"labels\": torch.tensor([pad(b[\"labels\"], -100) for b in batch])\n    }\n\n# =========================================================\n# 8Ô∏è‚É£ TRAINING\n# =========================================================\nargs = TrainingArguments(\n    output_dir=\"./fusion-sat-task\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    max_steps=4000,\n    logging_steps=100,\n    save_strategy=\"no\",\n    report_to=\"none\",\n    fp16=torch.cuda.is_available()\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    data_collator=collate_fn\n)\n\ntrainer.train()\n\n# =========================================================\n# 9Ô∏è‚É£ BLEU EVALUATION\n# =========================================================\nfrom sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw, n=100):\n    model.eval()\n    preds, refs = [], []\n    for ex in raw[:n]:\n        task_token = \"<|task_manglish|>\" if ex['dialect']==\"manglish\" else \"<|task_kelantanese|>\"\n        prompt = f\"{task_token}\\n<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            out = model.model.generate(**enc, max_new_tokens=40)\n        pred = tokenizer.decode(out[0], skip_special_tokens=True).split(\"<|target|>\")[-1].strip()\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n    return bleu.corpus_score(preds, [refs]).score\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw))\n\n# =========================================================\n# üîü INFERENCE\n# =========================================================\ndef translate(dialect, src):\n    task_token = \"<|task_manglish|>\" if dialect==\"manglish\" else \"<|task_kelantanese|>\"\n    p = f\"{task_token}\\n<|dialect|> {dialect}\\n<|source|> {src}\\n<|target|> \"\n    enc = tokenizer(p, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        out = model.model.generate(\n            **enc,\n            max_new_tokens=40,\n            do_sample=True,\n            top_p=0.9,\n            temperature=0.8,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=3\n        )\n    return tokenizer.decode(out[0], skip_special_tokens=True).split(\"<|target|>\")[-1].strip()\n\nprint(translate(\"kelantanese\", \"kuat\"))\nprint(translate(\"manglish\", \"I tak tau lah\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:24:10.909125Z","iopub.execute_input":"2026-01-05T14:24:10.909480Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2026-01-05 14:24:46.007446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767623086.237330      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767623086.302210      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767623086.856514      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767623086.856553      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767623086.856556      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767623086.856559      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Train / Eval: 1603 283\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec38cc39da8247c6a05061cbb44b0453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ba5dc9f662e4be890d277734be1acc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f0f3277711481a9b02f9334318610a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65e92213126042e5a573a11bc1fc9da1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83917a062d6420180ac33dcd59441c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f6f19247e54bab8f8e364ff1b5cb26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638846e0ff3d457789cc52b99ddab2a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93164c0af3bc4b1e98ae80eed3a7dd25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649fcfb0756648a997cb2421786f66f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/552M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f04c99f484417eac066af31d1ea589"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5dbe1b7d8f422f85f582de60649dbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/283 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387e037c8e39432cab34990976594fbb"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1464' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1464/4000 37:54 < 1:05:45, 0.64 it/s, Epoch 14.59/40]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>7.486800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.434200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.535600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.992300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.626600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.343400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.120600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.876200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.699800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.503700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.362100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.231700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.131200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.045600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"============Repaired ","metadata":{}},{"cell_type":"markdown","source":"# GPT-2 + Morph + Phoneme + SAT","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nfrom typing import List\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y accelerate transformers -q\n!pip install transformers==4.37.0 accelerate==0.27.2 -q\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\n\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\nprint(\"Samples:\", len(train_ds), len(eval_ds))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TOKENIZATION","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nSPECIAL_TOKENS = {\n    \"additional_special_tokens\": [\n        \"<|dialect|>\",\n        \"<|source|>\",\n        \"<|target|>\"\n    ]\n}\n\ntokenizer.add_special_tokens(SPECIAL_TOKENS)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nMAX_LEN = 256\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING (FAIR COMPARISON)","metadata":{}},{"cell_type":"code","source":"class MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n        self.model = BertModel.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        chars = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            chars.append(\n                self.char_emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n\n        return torch.cat([bert, torch.stack(chars)], dim=-1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BLEU EVALUATION (CORRECT)","metadata":{}},{"cell_type":"code","source":"class PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n\n        phones = \"pbtdkgmnshlrwji…õa…ô…îou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(\n                self.emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n        return torch.stack(outs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"class SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, mask):\n        B, L, H = h.size()\n\n        pos = torch.arange(L, device=h.device)\n        rel = pos[None, :] - pos[:, None] + 64\n        rel = rel.clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1, 2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(mask[:, None, :] == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        return torch.bmm(attn, v)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat  = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n\n        texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)texts = []\n        for ids in input_ids:\n            text = tokenizer.decode(ids, skip_special_tokens=True)\n            if \"<|source|>\" in text:\n                text = text.split(\"<|source|>\")[1].split(\"<|target|>\")[0]\n            texts.append(text.strip())\n\n\n        m = self.m_proj(self.morph(texts)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(texts)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n\nmodel = GPT2FusionSAT().to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(batch):\n    ids, masks, labels = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN\n        )\n\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\n        \"input_ids\": ids,\n        \"attention_mask\": masks,\n        \"labels\": labels\n    }\n\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\nmodel.train()\n\nfor step, batch in enumerate(train_ds):\n    if step >= 800:\n        break\n\n    batch = {k: v.unsqueeze(0).to(device) for k, v in batch.items()}\n\n    out = model(\n        input_ids=batch[\"input_ids\"],\n        attention_mask=batch[\"attention_mask\"],\n        labels=batch[\"labels\"]\n    )\n\n    loss = out.loss\n    loss.backward()\n\n    optimizer.step()\n    optimizer.zero_grad()\n\n    if step % 50 == 0:\n        print(f\"Step {step} | Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_input(model, dialect, source):\n    prompt = f\"<|dialect|> {dialect}\\n<|source|> {source}\\n<|target|> \"\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        out = model.model.generate(\n            **enc,\n            max_new_tokens=40,\n            num_beams=1,                 # üî¥ disable beam repetition\n            do_sample=True,              # üî¥ enable sampling\n            temperature=0.8,             # üî¥ soften logits\n            top_p=0.9,                   # üî¥ nucleus sampling\n            repetition_penalty=1.2,      # üî¥ critical\n            no_repeat_ngram_size=3,      # üî¥ critical\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text.split(\"<|target|>\")[-1].strip()\n\n\nprint(translate_input(model, \"kelantanese\", \"kuat\"))\nprint(translate_input(model, \"manglish\", \"I tak tau lah\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw_data, n=100):\n    model.eval()\n    preds, refs = [], []\n\n    for ex in raw_data[:n]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.model.generate(\n            **enc,\n            max_new_tokens=50,\n            do_sample=False,\n            num_beams=1,\n            pad_token_id=tokenizer.eos_token_id\n            )\n\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n\n    return bleu.corpus_score(preds, [refs]).score\n\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}