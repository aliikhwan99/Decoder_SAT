{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12287144,"sourceType":"datasetVersion","datasetId":7743669},{"sourceId":12315409,"sourceType":"datasetVersion","datasetId":7762636}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================\n# 0ï¸âƒ£ ENVIRONMENT (MATCHED VERSIONS â€“ IMPORTANT)\n# =========================================================\n!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate==0.26.1 datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nfrom typing import List\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1ï¸âƒ£ LOAD DATA\n# =========================================================\ndef load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\n# =========================================================\n# 2ï¸âƒ£ TOKENIZER (LEFT PAD â€“ REQUIRED FOR GPT)\n# =========================================================\nMODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntokenizer.add_special_tokens({\n    \"additional_special_tokens\": [\n        \"<|dialect|>\",\n        \"<|source|>\",\n        \"<|target|>\"\n    ]\n})\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nMAX_LEN = 256\n\n# =========================================================\n# 3ï¸âƒ£ MORPHOLOGY EMBEDDER (FROZEN)\n# =========================================================\nclass MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n        self.model = BertModel.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        char_feats = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            char_feats.append(\n                self.char_emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n\n        return torch.cat([bert, torch.stack(char_feats)], dim=-1)\n\n# =========================================================\n# 4ï¸âƒ£ PHONEME EMBEDDER\n# =========================================================\nclass PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n\n        phones = \"pbtdkgmnshlrwjiÉ›aÉ™É”ou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(\n                self.emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n        return torch.stack(outs)\n\n# =========================================================\n# 5ï¸âƒ£ UNSUPERVISED SYNTAX-AWARE ATTENTION\n# =========================================================\nclass SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, mask):\n        B, L, H = h.size()\n\n        pos = torch.arange(L, device=h.device)\n        rel = pos[None, :] - pos[:, None] + 64\n        rel = rel.clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1, 2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(mask[:, None, :] == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        return torch.bmm(attn, v)\n\n# =========================================================\n# 6ï¸âƒ£ GPT-2 FUSION + SAT MODEL (FIXED)\n# =========================================================\nclass GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat  = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n\n        # ğŸ”¥ CRITICAL FIX: SOURCE-ONLY TEXT\n        decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n        texts = []\n        for t in decoded:\n            if \"<|source|>\" in t:\n                t = t.split(\"<|source|>\")[1].split(\"<|target|>\")[0]\n            texts.append(t.strip())\n\n        m = self.m_proj(self.morph(texts)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(texts)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\nmodel = GPT2FusionSAT().to(device)\n\n# =========================================================\n# 7ï¸âƒ£ DATA PREPROCESSING\n# =========================================================\ndef preprocess(batch):\n    ids, masks, labels = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN\n        )\n\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\n        \"input_ids\": ids,\n        \"attention_mask\": masks,\n        \"labels\": labels\n    }\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n\n# =========================================================\n# 8ï¸âƒ£ TRAINING\n# =========================================================\nargs = TrainingArguments(\n    output_dir=\"./fusion-sat\",\n\n    # ====== BATCHING ======\n    per_device_train_batch_size=2,     # keep small (SAT is expensive)\n    gradient_accumulation_steps=8,      # effective batch = 16\n\n    # ====== OPTIMIZATION ======\n    learning_rate=2e-5,                 # lower LR for fusion + SAT\n    warmup_steps=800,                   # VERY important for SAT\n    max_steps=4000,                     # ğŸš€ huge training\n\n    # ====== STABILITY ======\n    lr_scheduler_type=\"cosine\",         # smoother than linear\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n\n    # ====== LOGGING ======\n    logging_steps=100,\n\n    # ====== CHECKPOINTING ======\n    save_strategy=\"no\",                 # avoid safetensors issues\n    evaluation_strategy=\"no\",\n\n    # ====== SYSTEM ======\n    report_to=\"none\",\n    fp16=torch.cuda.is_available(),\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\n# =========================================================\n# 9ï¸âƒ£ BLEU EVALUATION\n# =========================================================\nfrom sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw_data, n=100):\n    model.eval()\n    preds, refs = [], []\n\n    for ex in raw_data[:n]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.model.generate(\n                **enc,\n                max_new_tokens=50,\n                num_beams=4,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n\n    return bleu.corpus_score(preds, [refs]).score\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw))\n\n# =========================================================\n# ğŸ”Ÿ INFERENCE (NO REPETITION)\n# =========================================================\ndef translate_input(model, dialect, source):\n    prompt = f\"<|dialect|> {dialect}\\n<|source|> {source}\\n<|target|> \"\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        out = model.model.generate(\n            **enc,\n            max_new_tokens=40,\n            do_sample=True,\n            temperature=0.8,\n            top_p=0.9,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text.split(\"<|target|>\")[-1].strip()\n\nprint(translate_input(model, \"kelantanese\", \"kuat\"))\nprint(translate_input(model, \"manglish\", \"I tak tau lah\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T04:36:30.580624Z","iopub.execute_input":"2026-01-02T04:36:30.581264Z","iopub.status.idle":"2026-01-02T08:23:12.203466Z","shell.execute_reply.started":"2026-01-02T04:36:30.581225Z","shell.execute_reply":"2026-01-02T08:23:12.202733Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2026-01-02 04:37:05.459687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767328625.672213      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767328625.736459      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767328626.261836      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767328626.261884      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767328626.261887      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767328626.261889      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b16f2fd0ad10428c89b474dcb7cd8941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ddf5b1f321b4f05af746f99f78f4b66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"270311aec2794654aff372396d1a702d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b8d2921830b4920a71287e34081166c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b94e356727514a4296649001f91fb74f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"560be6a2d32c46578ec17253abcb3710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac5cbd4b89041d1812a19672add7904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b1c4f33c8f041fea663947ff0287eab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15068916dcca4880a3cd002f902acb5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/552M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30fce2c2a4fa4c60bee01bef28ab69c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa82b3bd0b69419987f1f4ba77febf9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/283 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d317b60307fb49f69717123a594de1e7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4000/4000 3:44:33, Epoch 79/80]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>23.161400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.108700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.676500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.544800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.455100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.392600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.359000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.337900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.319800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.312300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.291500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.328100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.271900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.262600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.253000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.245000</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.237700</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.233700</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.399000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.251400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.240400</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.222200</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.221900</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.214400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.213900</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.218100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.346500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.381000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.320800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.319000</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.330200</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.317100</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.322700</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.314500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.321600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.307900</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.321100</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.318800</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.314400</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.315000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"BLEU: 0.5587961109843743\nkelantanese\n kuat\n ajah, a ting barak pemel jampangk. sapau surpas say ha mau pun menes in say it melon to man teri belin\nmanglish\n I tak tau lah\n ik it kelising pas pan, puti and menkaring baras in a bulih belen to say pemus or man bersai. jalan law cerab do\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# GPT-2 + Morph + Phoneme + SAT","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y peft accelerate transformers -q\n!pip install -q transformers==4.37.0 accelerate datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nfrom typing import List\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y accelerate transformers -q\n!pip install transformers==4.37.0 accelerate==0.27.2 -q\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\n\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\nprint(\"Samples:\", len(train_ds), len(eval_ds))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TOKENIZATION","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nSPECIAL_TOKENS = {\n    \"additional_special_tokens\": [\n        \"<|dialect|>\",\n        \"<|source|>\",\n        \"<|target|>\"\n    ]\n}\n\ntokenizer.add_special_tokens(SPECIAL_TOKENS)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nMAX_LEN = 256\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING (FAIR COMPARISON)","metadata":{}},{"cell_type":"code","source":"class MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n        self.model = BertModel.from_pretrained(\n            \"imvladikon/charbert-bert-wiki\"\n        )\n\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=128,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        chars = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            chars.append(\n                self.char_emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n\n        return torch.cat([bert, torch.stack(chars)], dim=-1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BLEU EVALUATION (CORRECT)","metadata":{}},{"cell_type":"code","source":"class PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n\n        phones = \"pbtdkgmnshlrwjiÉ›aÉ™É”ou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(\n                self.emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n        return torch.stack(outs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"class SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, mask):\n        B, L, H = h.size()\n\n        pos = torch.arange(L, device=h.device)\n        rel = pos[None, :] - pos[:, None] + 64\n        rel = rel.clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1, 2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(mask[:, None, :] == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        return torch.bmm(attn, v)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat  = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n\n        texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)texts = []\n        for ids in input_ids:\n            text = tokenizer.decode(ids, skip_special_tokens=True)\n            if \"<|source|>\" in text:\n                text = text.split(\"<|source|>\")[1].split(\"<|target|>\")[0]\n            texts.append(text.strip())\n\n\n        m = self.m_proj(self.morph(texts)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(texts)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n\nmodel = GPT2FusionSAT().to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(batch):\n    ids, masks, labels = [], [], []\n\n    for d, s, t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LEN\n        )\n\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\n        \"input_ids\": ids,\n        \"attention_mask\": masks,\n        \"labels\": labels\n    }\n\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\nmodel.train()\n\nfor step, batch in enumerate(train_ds):\n    if step >= 800:\n        break\n\n    batch = {k: v.unsqueeze(0).to(device) for k, v in batch.items()}\n\n    out = model(\n        input_ids=batch[\"input_ids\"],\n        attention_mask=batch[\"attention_mask\"],\n        labels=batch[\"labels\"]\n    )\n\n    loss = out.loss\n    loss.backward()\n\n    optimizer.step()\n    optimizer.zero_grad()\n\n    if step % 50 == 0:\n        print(f\"Step {step} | Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_input(model, dialect, source):\n    prompt = f\"<|dialect|> {dialect}\\n<|source|> {source}\\n<|target|> \"\n    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        out = model.model.generate(\n            **enc,\n            max_new_tokens=40,\n            num_beams=1,                 # ğŸ”´ disable beam repetition\n            do_sample=True,              # ğŸ”´ enable sampling\n            temperature=0.8,             # ğŸ”´ soften logits\n            top_p=0.9,                   # ğŸ”´ nucleus sampling\n            repetition_penalty=1.2,      # ğŸ”´ critical\n            no_repeat_ngram_size=3,      # ğŸ”´ critical\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    return text.split(\"<|target|>\")[-1].strip()\n\n\nprint(translate_input(model, \"kelantanese\", \"kuat\"))\nprint(translate_input(model, \"manglish\", \"I tak tau lah\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw_data, n=100):\n    model.eval()\n    preds, refs = [], []\n\n    for ex in raw_data[:n]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.model.generate(\n            **enc,\n            max_new_tokens=50,\n            do_sample=False,\n            num_beams=1,\n            pad_token_id=tokenizer.eos_token_id\n            )\n\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n\n    return bleu.corpus_score(preds, [refs]).score\n\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}