{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12287144,"sourceType":"datasetVersion","datasetId":7743669},{"sourceId":12315409,"sourceType":"datasetVersion","datasetId":7762636}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT-2 + Morph + Phoneme + SAT","metadata":{}},{"cell_type":"code","source":"# ğŸ”¥ FORCE CLEAN PEFT + MATCH TRANSFORMERS\n!pip uninstall -y peft transformers accelerate -q\n!pip install -q transformers==4.37.0 accelerate==0.27.2 datasets sacrebleu epitran torch\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:53:28.113634Z","iopub.execute_input":"2026-01-01T14:53:28.113917Z","iopub.status.idle":"2026-01-01T14:53:46.547214Z","shell.execute_reply.started":"2026-01-01T14:53:28.113886Z","shell.execute_reply":"2026-01-01T14:53:46.546503Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.9/78.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================================================\n# 0ï¸âƒ£ ENVIRONMENT\n# =========================================================\n!pip install -q transformers==4.37.0 accelerate datasets sacrebleu epitran torch\n\nimport os, json, glob, random, torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import List\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# =========================================================\n# 1ï¸âƒ£ LOAD DATA (same as before)\n# =========================================================\ndef load_jsonl_from_dir(directory, dialect, src_key, tgt_key):\n    data = []\n    for path in glob.glob(f\"{directory}/*.jsonl\"):\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as f:\n            for line in f:\n                try:\n                    r = json.loads(line)\n                    if src_key in r and tgt_key in r:\n                        data.append({\n                            \"dialect\": dialect,\n                            \"source\": r[src_key],\n                            \"target\": r[tgt_key]\n                        })\n                except:\n                    pass\n    return data\n\nMANGLISH_DIR = \"/kaggle/input/filtered-manglish-1-8kv2\"\nKELANTAN_DIR = \"/kaggle/input/finaldialectdataset\"\n\ndata = (\n    load_jsonl_from_dir(MANGLISH_DIR, \"manglish\", \"text\", \"malay\") +\n    load_jsonl_from_dir(KELANTAN_DIR, \"kelantanese\", \"kelantanese\", \"stdMalay\")\n)\n\nrandom.shuffle(data)\nsplit = int(0.85 * len(data))\ntrain_raw = data[:split]\neval_raw  = data[split:]\n\ntrain_ds = Dataset.from_list(train_raw)\neval_ds  = Dataset.from_list(eval_raw)\n\n# =========================================================\n# 2ï¸âƒ£ TOKENIZER (LEFT PAD â€” IMPORTANT)\n# =========================================================\nMODEL_NAME = \"gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nMAX_LEN = 256\n\n# =========================================================\n# 3ï¸âƒ£ MORPHOLOGY EMBEDDER (FROZEN)\n# =========================================================\nclass MorphologyEmbedder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from transformers import BertTokenizer, BertModel\n\n        self.tokenizer = BertTokenizer.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        self.model = BertModel.from_pretrained(\"imvladikon/charbert-bert-wiki\")\n        for p in self.model.parameters():\n            p.requires_grad = False\n\n        self.char_emb = nn.Embedding(128, 64)\n        self.embedding_dim = self.model.config.hidden_size + 64\n\n    def forward(self, texts):\n        inputs = self.tokenizer(\n            texts, padding=True, truncation=True,\n            max_length=128, return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            bert = self.model(**inputs).last_hidden_state.mean(dim=1)\n\n        char_feats = []\n        for t in texts:\n            ids = [ord(c) if ord(c) < 128 else 0 for c in t[:64]]\n            ids += [0] * (64 - len(ids))\n            char_feats.append(\n                self.char_emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n\n        return torch.cat([bert, torch.stack(char_feats)], dim=-1)\n\n# =========================================================\n# 4ï¸âƒ£ PHONEME EMBEDDER\n# =========================================================\nclass PhonemeEmbedder(nn.Module):\n    def __init__(self, dim=256):\n        super().__init__()\n        import epitran\n        self.epi = epitran.Epitran(\"msa-Latn\")\n        phones = \"pbtdkgmnshlrwjiÉ›aÉ™É”ou\"\n        self.map = {p: i for i, p in enumerate(phones)}\n        self.pad = len(self.map)\n        self.emb = nn.Embedding(self.pad + 1, dim)\n\n    def forward(self, texts):\n        outs = []\n        for t in texts:\n            ps = [p for p in self.epi.transliterate(t) if p in self.map][:64]\n            ids = [self.map.get(p, self.pad) for p in ps]\n            ids += [self.pad] * (64 - len(ids))\n            outs.append(\n                self.emb(torch.tensor(ids, device=device)).mean(dim=0)\n            )\n        return torch.stack(outs)\n\n# =========================================================\n# 5ï¸âƒ£ UNSUPERVISED SYNTAX-AWARE ATTENTION\n# =========================================================\nclass SyntaxAwareAttention(nn.Module):\n    def __init__(self, hidden):\n        super().__init__()\n        self.rel_pos = nn.Embedding(128, hidden)\n        self.q = nn.Linear(hidden, hidden)\n        self.k = nn.Linear(hidden, hidden)\n        self.v = nn.Linear(hidden, hidden)\n\n    def forward(self, h, attn_mask):\n        B, L, H = h.size()\n        pos = torch.arange(L, device=h.device)\n        rel = pos[None, :] - pos[:, None] + 64\n        rel = rel.clamp(0, 127)\n\n        q = self.q(h)\n        k = self.k(h)\n        v = self.v(h)\n\n        scores = torch.bmm(q, k.transpose(1,2)) / (H ** 0.5)\n        scores += torch.einsum(\"bld,lrd->blr\", q, self.rel_pos(rel))\n        scores = scores.masked_fill(attn_mask[:,None,:] == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        return torch.bmm(attn, v)\n\n# =========================================================\n# 6ï¸âƒ£ GPT-2 FUSION MODEL\n# =========================================================\nclass GPT2FusionSAT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n        self.model.resize_token_embeddings(len(tokenizer))\n\n        h = self.model.config.hidden_size\n        self.morph = MorphologyEmbedder()\n        self.phon  = PhonemeEmbedder()\n\n        self.m_proj = nn.Linear(self.morph.embedding_dim, h)\n        self.p_proj = nn.Linear(256, h)\n\n        self.gate = nn.Linear(h * 3, h)\n        self.sat = SyntaxAwareAttention(h)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        tok = self.model.transformer.wte(input_ids)\n        texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n\n        m = self.m_proj(self.morph(texts)).unsqueeze(1).expand_as(tok)\n        p = self.p_proj(self.phon(texts)).unsqueeze(1).expand_as(tok)\n\n        g = torch.sigmoid(self.gate(torch.cat([tok, m, p], dim=-1)))\n        fused = tok + g * (m + p)\n\n        fused = fused + self.sat(fused, attention_mask)\n\n        return self.model(\n            inputs_embeds=fused,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\nmodel = GPT2FusionSAT().to(device)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:54:13.564484Z","iopub.execute_input":"2026-01-01T14:54:13.564808Z","iopub.status.idle":"2026-01-01T14:54:51.815513Z","shell.execute_reply.started":"2026-01-01T14:54:13.564779Z","shell.execute_reply":"2026-01-01T14:54:51.814855Z"}},"outputs":[{"name":"stderr","text":"2026-01-01 14:54:30.456241: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767279270.618154      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767279270.669197      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767279271.049697      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767279271.049729      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767279271.049731      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767279271.049734      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bcde285ab6744eb9434a2cbacf87070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410ac9ce1bb2404784270d8745f7015d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2516b37154c54aed9830db80e0a896be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe52536935a4bb2874bed8010acc287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d953256dce8b43a79ce69368559c86a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4baf67c7067041ebb064ab2159cf4576"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab11f9ce76d84dd8bf01e2c0c5a44992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75fc5d89efc44412b3a0da8a5c719277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f946b5de5cb142a8821f6acd05285ef6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/552M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727d1bd7768347f2a407d023e410cc72"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# TOKENIZATION","metadata":{}},{"cell_type":"code","source":"def preprocess(batch):\n    ids, masks, labels = [], [], []\n\n    for d,s,t in zip(batch[\"dialect\"], batch[\"source\"], batch[\"target\"]):\n        prompt = f\"<|dialect|> {d}\\n<|source|> {s}\\n<|target|> \"\n        full = prompt + t + tokenizer.eos_token\n\n        enc = tokenizer(\n            full, max_length=MAX_LEN,\n            truncation=True, padding=\"max_length\"\n        )\n\n        lab = enc[\"input_ids\"].copy()\n        p_len = len(tokenizer(prompt)[\"input_ids\"])\n        lab[:p_len] = [-100] * p_len\n\n        ids.append(enc[\"input_ids\"])\n        masks.append(enc[\"attention_mask\"])\n        labels.append(lab)\n\n    return {\"input_ids\": ids, \"attention_mask\": masks, \"labels\": labels}\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\neval_ds  = eval_ds.map(preprocess, batched=True, remove_columns=eval_ds.column_names)\n\ntrain_ds.set_format(\"torch\")\neval_ds.set_format(\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:55:32.834544Z","iopub.execute_input":"2026-01-01T14:55:32.834866Z","iopub.status.idle":"2026-01-01T14:55:33.550818Z","shell.execute_reply.started":"2026-01-01T14:55:32.834836Z","shell.execute_reply":"2026-01-01T14:55:33.550071Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff96f2811bad4ba481589aca9ebee689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/283 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482ce1057cc643938907393ccbc67868"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# TRAINING (FAIR COMPARISON)","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"./gpt2-fusion-sat\",\n    max_steps=200,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-5,\n    fp16=torch.cuda.is_available(),\n    logging_steps=20,\n    save_strategy=\"no\",\n    evaluation_strategy=\"no\",\n    report_to=\"none\"\n)\n\nTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    tokenizer=tokenizer\n).train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T14:55:37.952867Z","iopub.execute_input":"2026-01-01T14:55:37.953180Z","iopub.status.idle":"2026-01-01T15:01:10.331610Z","shell.execute_reply.started":"2026-01-01T14:55:37.953154Z","shell.execute_reply":"2026-01-01T15:01:10.331064Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 05:29, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>2.056800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.726600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.489200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.355800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.364000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.333600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.370200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.317400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.311900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.312900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.563834683895111, metrics={'train_runtime': 332.0214, 'train_samples_per_second': 9.638, 'train_steps_per_second': 0.602, 'total_flos': 0.0, 'train_loss': 0.563834683895111, 'epoch': 2.0})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# BLEU EVALUATION (CORRECT)","metadata":{}},{"cell_type":"code","source":"from sacrebleu.metrics import BLEU\nbleu = BLEU()\n\ndef evaluate_bleu(model, raw_data, num_samples=100):\n    model.eval()\n    preds, refs = [], []\n\n    for ex in raw_data[:num_samples]:\n        prompt = f\"<|dialect|> {ex['dialect']}\\n<|source|> {ex['source']}\\n<|target|> \"\n        enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            out = model.model.generate(\n                **enc,\n                max_new_tokens=60,\n                num_beams=4,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        pred = pred.split(\"<|target|>\")[-1].strip()\n\n        preds.append(pred)\n        refs.append(ex[\"target\"])\n\n    return bleu.corpus_score(preds, [refs]).score\n\nprint(\"BLEU:\", evaluate_bleu(model, eval_raw, 100))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-01T15:10:22.659365Z","iopub.execute_input":"2026-01-01T15:10:22.660146Z","iopub.status.idle":"2026-01-01T15:11:36.617433Z","shell.execute_reply.started":"2026-01-01T15:10:22.660115Z","shell.execute_reply":"2026-01-01T15:11:36.616564Z"}},"outputs":[{"name":"stdout","text":"BLEU: 0.05297073696449619\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}